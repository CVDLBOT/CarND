{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavioral Cloning** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavrioal Cloning Project**\n",
    "\n",
    "** The goals / steps of this project are the following: **\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** My project includes the following files: **\n",
    "* model.py containing the script to create and train the model\n",
    "* model.ipynb containing the jupyter notebook version of model.py to create and train the model.\n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup_report.ipynb and writeup_report.pdf summarizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Testing **\n",
    "\n",
    "* Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing:\n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n",
    "* Saving the autonomous run can be done by executing:\n",
    "```sh\n",
    "python drive.py model.h5 <filename>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I used the same network architecture proposed in NVIDIA's \"End to End Learning for Self-Driving Cars\" paper.\n",
    "\n",
    "### Model Architecture \n",
    "\n",
    "The model is exactly the same as the proposed NVIDIA model, but I added a dropout layer before the second last layer, to prevent overfitting and for the model to generalize better.\n",
    "\n",
    "**Layer 1: Normalization Layer.** <br>\n",
    "Kernel: nil <br>\n",
    "Stride: nil <br>\n",
    "The output shape: 66x200x3. <br>\n",
    "\n",
    "**Layer 2: Convolutional.** <br>\n",
    "Kernel: 5x5x3x24<br>\n",
    "Stride: 2x2 <br>\n",
    "The output shape: 31x98x24. <br>\n",
    "\n",
    "**Layer 3: Convolutional.** <br>\n",
    "Kernel: 5x5x24x36<br>\n",
    "Stride: 2x2 <br>\n",
    "The output shape: 14x47x36. <br>\n",
    "\n",
    "**Layer 4: Convolutional.** <br>\n",
    "Kernel: 5x5x36x54<br>\n",
    "Stride: 2x2 <br>\n",
    "The output shape: 5x22x54. <br>\n",
    "\n",
    "**Layer 5: Convolutional.** <br>\n",
    "Kernel: 3x3x54x64<br>\n",
    "Stride: 1x1 <br>\n",
    "The output shape: 3x20x64. <br>\n",
    "\n",
    "**Layer 6: Convolutional.** <br>\n",
    "Kernel: 3x3x64x64<br>\n",
    "Stride: 1x1 <br>\n",
    "The output shape: 1x18x64. <br>\n",
    "\n",
    "**Flatten.** <br>\n",
    "Flattens the output shape of the final pooling layer such that it's 1D instead of 3D. <br>\n",
    "Output = 1152\n",
    "\n",
    "**Layer 1: Fully Connected.** <br>\n",
    "Layer Operation = 1x1152 x 1152x1164 <br>\n",
    "Layer Output = 1x1164 <br>\n",
    "\n",
    "**Activation.** <br>\n",
    "ReLU activation.\n",
    "\n",
    "**Layer 2: Fully Connected.** <br>\n",
    "Layer Operation = 1x1164 x 1164x100 <br>\n",
    "Layer Output =  1x100\n",
    "\n",
    "**Activation.** <br>\n",
    "ReLU activation.\n",
    "\n",
    "**Layer 3: Fully Connected.** <br>\n",
    "Layer Operation = 1x100 x 100x50 <br>\n",
    "Layer Output =  1x50\n",
    "\n",
    "**Activation.** <br>\n",
    "ReLU activation.\n",
    "\n",
    "**Layer 4: Dropout Layer ** <br>\n",
    "\n",
    "**Layer 5: Fully Connected.** <br>\n",
    "Layer Operation = 1x50 x 50x10 <br>\n",
    "Layer Output =  1x10\n",
    "\n",
    "**Activation.** <br>\n",
    "ReLU activation.\n",
    "\n",
    "**Layer 6: Fully Connected.** <br>\n",
    "Layer Operation = 1x10 x 10x1 <br>\n",
    "Layer Output =  1\n",
    "\n",
    "#### Output\n",
    "1 outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cnn.png)\n",
    "Source: End to End Learning for Self-Driving Cars (NVIDIA Corporation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Strategy\n",
    "\n",
    "##### Creation of the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Initially, I just used the sample training data provided. **\n",
    "* First, I just used the image data from center camera, and was met with very limited success. The car would barely, move before crashing onto the side. \n",
    "    \n",
    "* Then, I included the image data from both left and right cameras. I included a correction factor to the steering data, when left and right image data was used. This correction factor had to be fine-tuned. The best result I got was with a correction factor of 0.05.\n",
    "    \n",
    "* I was able to make the car go past the first curve, but It ended up crashing in the next one. I tried a lot of tuning the network parameters, but was still met with limited success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Then I collected my own dataset. **\n",
    "* It took sometime to collect the dataset, since I had to get used to controlling the car in the simulator.\n",
    "    \n",
    "* Initially, I only collected the data driving only on the center of the road as much as possible. (3-Laps)\n",
    "   \n",
    "* After, following the same stratergy of including the left and right cameras and taking into the steering correction. The car was able to drive close to half-way before crashing at the turn after bridge. This was mainly, because I have not yet collected the data for the network to learn, when they are close to edge.\n",
    "    \n",
    "* As a final stratergy, I collected the data for recovering driving from the sides and turns. I also collected a single lap of center driving data from  track two to make a more generalized model. The final collected dataset is given in the IMG folder and video output 1 is shown below.\n",
    "    \n",
    "* After training the model, My car was able to drive smoothly across the lap, and even though it side tracked at times, it was able to recover back to the center. \n",
    "\n",
    "* The trained weights have been included in set-1. I have included the output in video 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Lastly, I combined my collected dataset and the sample provided dataset.** \n",
    "\n",
    "* This was done in order to see, if the driving could be made more efficient.\n",
    "\n",
    "* The car was able to drive without crashing, I have included the output in video-3. The car was much more at the center.\n",
    "\n",
    "* The trained weights have been included in set-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "\n",
    "#### Type of optimizer: \n",
    "Adaptive Moment Estimation (Adam) Optimizer was to used to optimize the model. \n",
    "The learning rate was set to 0.0001. \n",
    "The other parameters for adam optimizer was set as the deafult parameters. \n",
    "Adam optimizer was used because it works well in practice and compares favorably to other adaptive learning-method algorithms. In addition to storing an exponentially decaying average of past squared gradient like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients similar to momentum.\n",
    "\n",
    "#### Cost Function:\n",
    "Mean Squared Error.\n",
    "\n",
    "#### Batch size: \n",
    "I used the batch size of 256. I tried 128 and 64 batch sized too, but 256 was giving me a better result.\n",
    "\n",
    "#### Epochs:\n",
    "The model was trained for 5 epochs. I found that around 4 or 5 epochs, the validation loss was staying almost constant. Hence, I selected the model to train for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Collected Dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collected_dataset = 'examples/IMG.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"200\" controls>\n",
       "  <source src=\"examples/IMG.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"200\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(collected_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Autonomous Driving after training only on the collected dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car_output1 = 'examples/run1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"200\" controls>\n",
       "  <source src=\"examples/run1.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"200\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(car_output1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Autonomous Driving after training on both collected dataset and sample training dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car_output2 = 'examples/run2.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"200\" controls>\n",
       "  <source src=\"examples/run2.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"200\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(car_output2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
